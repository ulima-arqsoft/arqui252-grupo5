> [0. Acerca del Grupo](../../0.md) â€º [0.6. Temas Individuales (Parte 1)](../0.6.md) â€º [0.6.1. Integrante 1](0.6.1.md)

# 0.6.1. Crisbeth Sovero

# Inteligencia Artificial - Large Language Models (LLM)

## 1. IntroducciÃ³n: 
### A. Concepto de Large Language Models
Los Large Language Models son redes neuronales entrenadas con cantidades masivas de texto y estÃ¡n diseÃ±adas para comprender, generar y razonar con lenguaje natural.

### B. Arquitectura y Fundamentos

**Arquitectura Transformer:** Permite a los LLM analizar todas las palabras de una oraciÃ³n a la vez (*self-attention*), entendiendo relaciones complejas entre ellas y mejorando la predicciÃ³n de palabras y generaciÃ³n de texto.

**Embedding:** Representaciones numÃ©ricas de informaciÃ³n (texto, imÃ¡genes, audio, etc.) en un espacio matemÃ¡tico. Convierten palabras, frases o documentos en vectores (listas de nÃºmeros) que capturan su significado.

### C. Modelos Representativos
Existen diversos modelos que demuestran las capacidades de esta arquitectura:

- **BERT (Google):** Modelo de comprensiÃ³n de lenguaje entrenado para entender el contexto bidireccional de las palabras en una oraciÃ³n, Ãºtil para tareas de anÃ¡lisis y bÃºsqueda.  
- **GPT-3:** Genera texto coherente y creativo a partir de indicaciones, capaz de completar oraciones, redactar y responder preguntas.  
- **ROBERTa (Facebook):** Variante optimizada de BERT que mejora la comprensiÃ³n del lenguaje mediante mayor cantidad de datos y entrenamiento mÃ¡s largo, logrando mejor desempeÃ±o en tareas de NLP.  
- **LLAMA (Meta):** Optimizado para eficiencia y flexibilidad, permite entrenar modelos grandes con menos recursos manteniendo un buen desempeÃ±o en diversas tareas de NLP.

---

## 2. Arquitectura de una AplicaciÃ³n Impulsada por LLM
La integraciÃ³n efectiva de estos modelos requiere una Arquitectura de una aplicaciÃ³n impulsada por Modelos de Lenguaje Extensos (LLM).  
Esta arquitectura define cÃ³mo integramos estos modelos dentro de un sistema mayor.

### A. OrquestaciÃ³n y Beneficios
La OrquestaciÃ³n es un proceso diseÃ±ado para optimizar y facilitar la integraciÃ³n, la gestiÃ³n y la optimizaciÃ³n de estos modelos para crear aplicaciones de IA escalables.

**Su propÃ³sito es garantizar que los modelos de lenguaje funcionen de manera eficiente y coherente dentro de un sistema mayor.**

**Beneficios de la OrquestaciÃ³n:**  
- Mejora del Rendimiento: Optimizar el rendimiento de las aplicaciones de IA.  
- Control en Tiempo Real: Asegurar el control de la salida (*output*) en tiempo real.  
- Interacciones con API: Facilitar interacciones mÃ¡s fluidas con la API.  
- Eficiencia: Asegurar la eficiencia de costos y la adaptabilidad en sistemas de IA en evoluciÃ³n.

### B. Frameworks de OrquestaciÃ³n
Estos frameworks son cruciales para implementar patrones como **RAG (Retrieval-Augmented Generation)** y **MCP (Model Combination Pattern):**

| Framework | CaracterÃ­sticas Clave | Casos de Uso |
|------------|------------------------|--------------|
| **LangChain** | Conectar LLMs con bases de datos, APIs, documentos, web scraping. Implementar RAG fÃ¡cilmente. Crear "agentes" que toman decisiones basadas en mÃºltiples pasos. | Apps que necesitan respuestas basadas en datos internos o externos, chatbots avanzados, asistentes inteligentes. |
| **LlamaIndex** | Estructurar informaciÃ³n y hacer bÃºsquedas semÃ¡nticas sobre documentos para alimentar un LLM. Crear Ã­ndices de datos (documentos, PDFs, bases de datos). Consultas inteligentes con contexto relevante. | IntegraciÃ³n de LLMs con grandes volÃºmenes de documentos internos, como soporte, legal o contenido acadÃ©mico. |
| **Haystack** | ConstrucciÃ³n de sistemas de bÃºsqueda y QA (*question answering*). Indexar documentos, integrar motores de bÃºsqueda. Pipelines de procesamiento, incluyendo extracciÃ³n, filtrado y generaciÃ³n de respuestas. | Ideal para entornos productivos orientados a bÃºsqueda y anÃ¡lisis semÃ¡ntico. |

---

## 3. Decisiones de ImplementaciÃ³n y Despliegue
La elecciÃ³n entre usar modelos localmente o a travÃ©s de APIs externas es una decisiÃ³n arquitectÃ³nica clave.

### A. Usar Modelo LLM Localmente

| Ventajas | Desventajas |
|-----------|--------------|
| Independencia: No se depende de un servicio externo (Hugging Face, OpenAI). | Requiere recursos (RAM, GPU, almacenamiento). |
| Sin costo por request: No se paga por cada llamada a la API. | Mantenimiento: Debes actualizar, optimizar y desplegar tÃº mismo los modelos. |
| Privacidad: Los datos no salen del servidor/PC. | Escalabilidad limitada: El servidor puede colapsar si no estÃ¡ bien dimensionado para alta carga. |
| Control total: Se puede *fine-tunear* el modelo. |  |

### B. Usando APIs Externas (OpenAI, Hugging Face)

| Ventajas | Desventajas |
|-----------|--------------|
| FÃ¡cil integraciÃ³n: Solo se consume un endpoint REST. | Costo por uso: Pago por tokens o requests. |
| Escalable: La nube se encarga de manejar la carga, GPUs y actualizaciones. | Dependencia externa: Si el servicio cae o cambian precios, el sistema se afecta. |
| Acceso a modelos grandes (GPT-4, LLAMA-3, Claude, Mistral) sin necesidad de descargarlos. | Privacidad: Los datos viajan a servidores externos. |
| Menor inversiÃ³n inicial. |  |


### RAG (Retrieval-Augmented Generation)

**DefiniciÃ³n:**  
El patrÃ³n **RAG (Retrieval-Augmented Generation)** combina la capacidad generativa de los LLM con la recuperaciÃ³n de informaciÃ³n desde fuentes externas (bases de datos, documentos, APIs, etc.).  
En lugar de depender Ãºnicamente del conocimiento del modelo, RAG busca datos relevantes antes de generar una respuesta, mejorando la precisiÃ³n y actualidad de la informaciÃ³n.

#### Arquitectura del flujo RAG:
1. **Consulta del usuario:** El sistema recibe una pregunta o instrucciÃ³n.  
2. **BÃºsqueda (Retrieval):** Un mÃ³dulo busca fragmentos de informaciÃ³n relevantes desde un repositorio (por ejemplo, una base vectorial).  
3. **FusiÃ³n contextual:** Los datos recuperados se incorporan al *prompt* del LLM como contexto adicional.  
4. **GeneraciÃ³n (Generation):** El LLM produce una respuesta fundamentada en los datos externos y su conocimiento previo.

#### Ventajas:
- Aumenta la **precisiÃ³n y confiabilidad** del modelo.  
- Permite **actualizar el conocimiento** sin reentrenar el modelo.  
- Facilita la **personalizaciÃ³n** con datos privados o corporativos.  

#### Ejemplo de uso:
> Un chatbot empresarial puede usar RAG para responder consultas basadas en manuales internos, polÃ­ticas o documentos legales de la compaÃ±Ã­a.  

#### Frameworks comunes:
- **LangChain + FAISS / ChromaDB** para la indexaciÃ³n y bÃºsqueda vectorial.  
- **LlamaIndex** para estructurar y consultar colecciones documentales.  

---

### MCP (Model Combination Pattern)

**DefiniciÃ³n:**  
El patrÃ³n **MCP (Model Combination Pattern)** consiste en **combinar varios modelos de IA o LLMs** â€”cada uno especializado en una tareaâ€” para lograr un comportamiento mÃ¡s robusto y flexible.  
Este patrÃ³n se basa en la colaboraciÃ³n entre modelos que se complementan, en lugar de depender de un solo modelo â€œgeneralistaâ€.

#### Ejemplo de combinaciÃ³n:
- Un modelo de **clasificaciÃ³n** identifica la intenciÃ³n del usuario.  
- Un modelo de **extracciÃ³n de informaciÃ³n** obtiene los datos clave.  
- Un modelo **generativo** (como GPT o LLAMA) redacta la respuesta final con estilo natural.

#### Beneficios:
- **Modularidad:** Cada modelo cumple una funciÃ³n especÃ­fica.  
- **Escalabilidad:** Es posible agregar o reemplazar modelos sin rediseÃ±ar todo el sistema.  
- **OptimizaciÃ³n de recursos:** Se usan modelos ligeros para tareas simples y grandes para las complejas.  

#### Arquitectura tÃ­pica MCP:
1. **Router o Coordinador:** Decide quÃ© modelo usar segÃºn el tipo de solicitud.  
2. **Submodelos especializados:** Ejecutan sus tareas especÃ­ficas.  
3. **FusiÃ³n de resultados:** Se combinan las salidas para formar una respuesta coherente.  

#### Frameworks y herramientas Ãºtiles:
- **LangChain Agents** o **OpenDevin** para coordinar modelos y herramientas.  
- **Prompt orchestration pipelines** para dirigir las solicitudes entre modelos.  

---

## ğŸ‘¾ Configuraciones TÃ©cnicas y Entorno de Desarrollo

A continuaciÃ³n, se detallan las configuraciones tÃ©cnicas necesarias para implementar y ejecutar arquitecturas basadas en **RAG**.

### A. Requisitos del Entorno

**VersiÃ³n recomendada de Python:** `>=3.10`  

**Dependencias principales:**

```bash
pip install streamlit langchain faiss-cpu pypdf
```

[ğŸ  Home](../../../README.md) | [Siguiente â¡ï¸](../0.6.2/0.6.2.md)
