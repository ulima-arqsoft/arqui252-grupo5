> [0. Acerca del Grupo](../../0.md) › [0.6. Temas Individuales (Parte 1)](../0.6.md) › [0.6.1. Integrante 1](0.6.1.md)

# 0.6.1. Crisbeth Sovero

# Inteligencia Artificial - Large Language Models (LLM)

## 1. Introducción: 
### A. Concepto de Large Language Models
Los Large Language Models son redes neuronales entrenadas con cantidades masivas de texto y están diseñadas para comprender, generar y razonar con lenguaje natural.

### B. Arquitectura y Fundamentos

**Arquitectura Transformer:** Permite a los LLM analizar todas las palabras de una oración a la vez (*self-attention*), entendiendo relaciones complejas entre ellas y mejorando la predicción de palabras y generación de texto.

**Embedding:** Representaciones numéricas de información (texto, imágenes, audio, etc.) en un espacio matemático. Convierten palabras, frases o documentos en vectores (listas de números) que capturan su significado.

### C. Modelos Representativos
Existen diversos modelos que demuestran las capacidades de esta arquitectura:

- **BERT (Google):** Modelo de comprensión de lenguaje entrenado para entender el contexto bidireccional de las palabras en una oración, útil para tareas de análisis y búsqueda.  
- **GPT-3:** Genera texto coherente y creativo a partir de indicaciones, capaz de completar oraciones, redactar y responder preguntas.  
- **ROBERTa (Facebook):** Variante optimizada de BERT que mejora la comprensión del lenguaje mediante mayor cantidad de datos y entrenamiento más largo, logrando mejor desempeño en tareas de NLP.  
- **LLAMA (Meta):** Optimizado para eficiencia y flexibilidad, permite entrenar modelos grandes con menos recursos manteniendo un buen desempeño en diversas tareas de NLP.

---

## 2. Arquitectura de una Aplicación Impulsada por LLM
La integración efectiva de estos modelos requiere una Arquitectura de una aplicación impulsada por Modelos de Lenguaje Extensos (LLM).  
Esta arquitectura define cómo integramos estos modelos dentro de un sistema mayor.

### A. Orquestación y Beneficios
La Orquestación es un proceso diseñado para optimizar y facilitar la integración, la gestión y la optimización de estos modelos para crear aplicaciones de IA escalables.

**Su propósito es garantizar que los modelos de lenguaje funcionen de manera eficiente y coherente dentro de un sistema mayor.**

**Beneficios de la Orquestación:**  
- Mejora del Rendimiento: Optimizar el rendimiento de las aplicaciones de IA.  
- Control en Tiempo Real: Asegurar el control de la salida (*output*) en tiempo real.  
- Interacciones con API: Facilitar interacciones más fluidas con la API.  
- Eficiencia: Asegurar la eficiencia de costos y la adaptabilidad en sistemas de IA en evolución.

### B. Frameworks de Orquestación
Estos frameworks son cruciales para implementar patrones como **RAG (Retrieval-Augmented Generation)** y **MCP (Model Combination Pattern):**

| Framework | Características Clave | Casos de Uso |
|------------|------------------------|--------------|
| **LangChain** | Conectar LLMs con bases de datos, APIs, documentos, web scraping. Implementar RAG fácilmente. Crear "agentes" que toman decisiones basadas en múltiples pasos. | Apps que necesitan respuestas basadas en datos internos o externos, chatbots avanzados, asistentes inteligentes. |
| **LlamaIndex** | Estructurar información y hacer búsquedas semánticas sobre documentos para alimentar un LLM. Crear índices de datos (documentos, PDFs, bases de datos). Consultas inteligentes con contexto relevante. | Integración de LLMs con grandes volúmenes de documentos internos, como soporte, legal o contenido académico. |
| **Haystack** | Construcción de sistemas de búsqueda y QA (*question answering*). Indexar documentos, integrar motores de búsqueda. Pipelines de procesamiento, incluyendo extracción, filtrado y generación de respuestas. | Ideal para entornos productivos orientados a búsqueda y análisis semántico. |

---

## 3. Decisiones de Implementación y Despliegue
La elección entre usar modelos localmente o a través de APIs externas es una decisión arquitectónica clave.

### A. Usar Modelo LLM Localmente

| Ventajas | Desventajas |
|-----------|--------------|
| Independencia: No se depende de un servicio externo (Hugging Face, OpenAI). | Requiere recursos (RAM, GPU, almacenamiento). |
| Sin costo por request: No se paga por cada llamada a la API. | Mantenimiento: Debes actualizar, optimizar y desplegar tú mismo los modelos. |
| Privacidad: Los datos no salen del servidor/PC. | Escalabilidad limitada: El servidor puede colapsar si no está bien dimensionado para alta carga. |
| Control total: Se puede *fine-tunear* el modelo. |  |

### B. Usando APIs Externas (OpenAI, Hugging Face)

| Ventajas | Desventajas |
|-----------|--------------|
| Fácil integración: Solo se consume un endpoint REST. | Costo por uso: Pago por tokens o requests. |
| Escalable: La nube se encarga de manejar la carga, GPUs y actualizaciones. | Dependencia externa: Si el servicio cae o cambian precios, el sistema se afecta. |
| Acceso a modelos grandes (GPT-4, LLAMA-3, Claude, Mistral) sin necesidad de descargarlos. | Privacidad: Los datos viajan a servidores externos. |
| Menor inversión inicial. |  |


### RAG (Retrieval-Augmented Generation)

**Definición:**  
El patrón **RAG (Retrieval-Augmented Generation)** combina la capacidad generativa de los LLM con la recuperación de información desde fuentes externas (bases de datos, documentos, APIs, etc.).  
En lugar de depender únicamente del conocimiento del modelo, RAG busca datos relevantes antes de generar una respuesta, mejorando la precisión y actualidad de la información.

#### Arquitectura del flujo RAG:
1. **Consulta del usuario:** El sistema recibe una pregunta o instrucción.  
2. **Búsqueda (Retrieval):** Un módulo busca fragmentos de información relevantes desde un repositorio (por ejemplo, una base vectorial).  
3. **Fusión contextual:** Los datos recuperados se incorporan al *prompt* del LLM como contexto adicional.  
4. **Generación (Generation):** El LLM produce una respuesta fundamentada en los datos externos y su conocimiento previo.

#### Ventajas:
- Aumenta la **precisión y confiabilidad** del modelo.  
- Permite **actualizar el conocimiento** sin reentrenar el modelo.  
- Facilita la **personalización** con datos privados o corporativos.  

#### Ejemplo de uso:
> Un chatbot empresarial puede usar RAG para responder consultas basadas en manuales internos, políticas o documentos legales de la compañía.  

#### Frameworks comunes:
- **LangChain + FAISS / ChromaDB** para la indexación y búsqueda vectorial.  
- **LlamaIndex** para estructurar y consultar colecciones documentales.  

---

### MCP (Model Combination Pattern)

**Definición:**  
El patrón **MCP (Model Combination Pattern)** consiste en **combinar varios modelos de IA o LLMs** —cada uno especializado en una tarea— para lograr un comportamiento más robusto y flexible.  
Este patrón se basa en la colaboración entre modelos que se complementan, en lugar de depender de un solo modelo “generalista”.

#### Ejemplo de combinación:
- Un modelo de **clasificación** identifica la intención del usuario.  
- Un modelo de **extracción de información** obtiene los datos clave.  
- Un modelo **generativo** (como GPT o LLAMA) redacta la respuesta final con estilo natural.

#### Beneficios:
- **Modularidad:** Cada modelo cumple una función específica.  
- **Escalabilidad:** Es posible agregar o reemplazar modelos sin rediseñar todo el sistema.  
- **Optimización de recursos:** Se usan modelos ligeros para tareas simples y grandes para las complejas.  

#### Arquitectura típica MCP:
1. **Router o Coordinador:** Decide qué modelo usar según el tipo de solicitud.  
2. **Submodelos especializados:** Ejecutan sus tareas específicas.  
3. **Fusión de resultados:** Se combinan las salidas para formar una respuesta coherente.  

#### Frameworks y herramientas útiles:
- **LangChain Agents** o **OpenDevin** para coordinar modelos y herramientas.  
- **Prompt orchestration pipelines** para dirigir las solicitudes entre modelos.  

---

## 👾 Configuraciones Técnicas y Entorno de Desarrollo

A continuación, se detallan las configuraciones técnicas necesarias para implementar y ejecutar arquitecturas basadas en **RAG**.

### A. Requisitos del Entorno

**Versión recomendada de Python:** `>=3.10`  

**Dependencias principales:**

```bash
pip install streamlit langchain faiss-cpu pypdf
```

[🏠 Home](../../../README.md) | [Siguiente ➡️](../0.6.2/0.6.2.md)
