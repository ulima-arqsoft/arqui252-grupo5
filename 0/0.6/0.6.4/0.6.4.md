> [0. Acerca del Grupo](../../0.md) â€º [0.6. Temas Individuales (Parte 1)](../0.6.md) â€º [0.6.4. Integrante 4](0.6.4.md)

# 0.6.4. Ana Meza
---
# ğŸ§  Desarrollo Conceptual â€” OrquestaciÃ³n de Contenedores

## âœ… Â¿QuÃ© es?
La **orquestaciÃ³n de contenedores** es el conjunto de prÃ¡cticas y componentes que automatizan el **despliegue, escalado, actualizaciÃ³n y operaciÃ³n** de aplicaciones en contenedores sobre un **clÃºster**. Funciona de forma **declarativa**: defines el *estado deseado* (p. ej., â€œ3 rÃ©plicas de este servicioâ€) y el orquestador lo mantiene mediante **reconciliaciÃ³n**, gestionando redes, almacenamiento, secretos, polÃ­ticas y observabilidad.

### ğŸ–‡ï¸ Â¿QuÃ© es un clÃºster?
Un **clÃºster** es un **conjunto de mÃ¡quinas** (fÃ­sicas o virtuales) que operan como un sistema Ãºnico:
- **Plano de control**: decide y coordina (API, scheduler, controladores).
- **Nodos de trabajo** (*workers*): ejecutan los contenedores/pods.
- Comparten **red** y, cuando aplica, **almacenamiento**, habilitando **alta disponibilidad** y **escalado** al distribuir la carga y reemplazar instancias con fallas automÃ¡ticamente.

---

## ğŸŒŸ Â¿Por quÃ© usar un orquestador?
- **Alta disponibilidad y autorreparaciÃ³n** (recrea pods si fallan).
- **Escalabilidad horizontal automÃ¡tica** (HPA/VPA) segÃºn carga o mÃ©tricas.
- **Despliegues seguros y sin caÃ­da**: *rolling*, *blue/green*, *canary* y **rollback**.
- **EstandarizaciÃ³n operativa** (red, volÃºmenes, secretos, polÃ­ticas).
- **Uso eficiente de recursos** (bin-packing con *requests/limits*).
- **Observabilidad** integrada (mÃ©tricas, logs, trazas) para SRE.

---

## ğŸŸ© CuÃ¡ndo conviene
- **Microservicios** o mÃºltiples servicios que deben alinearse.
- **Picos de demanda** y necesidad de **autoscaling**.
- **Entornos multi-equipo** con cuotas, aislamiento y polÃ­ticas.
- **Entrega continua** con despliegues frecuentes y *rollback* Ã¡gil.
- **Plataformas internas** (PaaS) que hospedan varios productos.

## ğŸŸ¥ CuÃ¡ndo *no* conviene
- Proyectos **pequeÃ±os/estÃ¡ticos** (un servicio simple sin escalar).
- Equipos **sin capacidad operativa** para administrar un clÃºster.
- Workloads con **dependencias muy host-acopladas**.
- Requisitos de **tiempo real duro** donde la sobrecarga no es aceptable.

---

## âš ï¸ DesafÃ­os y riesgos (a gestionar)
- **Curva de aprendizaje** y complejidad operativa.
- **Costos** por sobredimensionamiento o *requests/limits* mal definidos.
- **Seguridad**: exfiltraciÃ³n de secretos, imÃ¡genes vulnerables, red abierta.
- **Observabilidad** insuficiente complica diagnÃ³stico y *MTTR*.

---

## ğŸ§© Componentes y conceptos clave (agnÃ³sticos)
| Capa | QuÃ© hace | Ejemplos |
|---|---|---|
| **Plano de control** | Mantiene estado deseado y programa cargas | API Server, Scheduler, Controladores |
| **Plano de datos** | Ejecuta contenedores y expone red/volÃºmenes | Nodos de trabajo + runtime |
| **Red (CNI)** | Conectividad y polÃ­ticas L3/L4 | Calico, Cilium, Flannel |
| **Almacenamiento (CSI)** | ProvisiÃ³n de volÃºmenes y *claims* | HostPath, NFS, Ceph |
| **Identidad y acceso** | Permisos por rol | RBAC, ServiceAccounts |
| **Observabilidad** | MÃ©tricas, logs, trazas | metrics-server, Prometheus/Grafana, Loki/ELK, OpenTelemetry |

---

## ğŸ› ï¸ Objetos/patrones tÃ­picos
| Objeto/PatrÃ³n | QuÃ© resuelve | Ejemplo |
|---|---|---|
| **Deployment** | RÃ©plicas y estrategia de actualizaciÃ³n | 3 pods con *rolling update* 25%/25% |
| **Service** | Descubrimiento y balanceo L4 | `ClusterIP` (interno), `NodePort/LoadBalancer` (externo) |
| **Ingress / Gateway** | Enrutamiento HTTP/HTTPS L7 | Ruta `/api` â†’ servicio `backend` con TLS |
| **ConfigMap / Secret** | Config y credenciales fuera del cÃ³digo | `APP_MODE=prod`, token base64 |
| **Job / CronJob** | Tareas puntuales o programadas | ETL diario 03:00 |
| **HPA / VPA** | Autoscaling por mÃ©tricas o ajuste de recursos | Escalar si CPU > 70% |
| **NetworkPolicy** | Control de trÃ¡fico entre pods | Solo `frontend` â†’ `api` |

---

## ğŸ” Estrategias de despliegue
| Estrategia | Ãšsala cuandoâ€¦ | Trade-off |
|---|---|---|
| **Rolling Update** | Cambios frecuentes con bajo riesgo | MÃ­nimo riesgo; propagaciÃ³n gradual |
| **Recreate** | Incompatibilidad de estado/sesiones | Downtime breve |
| **Blue/Green** | Cambios grandes; *rollback* instantÃ¡neo | Requiere doble capacidad |
| **Canary** | Validar con % pequeÃ±o de trÃ¡fico | Mayor complejidad de ruteo/telemetrÃ­a |

---

## ğŸ“ MÃ©tricas clave de operaciÃ³n
- **Disponibilidad (SLA/SLO)** y **error rate** por servicio.  
- **Latencias** P50/P95/P99 en endpoints crÃ­ticos.  
- **CPU/RAM** usados vs *requests/limits* (evitar *throttling* y **OOMKilled**).  
- **Tiempo de despliegue** y **tasa de rollback**.  
- **MTTR** ante fallas y **saturaciÃ³n** (colas, conexiones, lÃ­mites de pod/nodo).

---

## ğŸ“š Glosario rÃ¡pido
- **Pod**: unidad mÃ­nima de ejecuciÃ³n (uno o mÃ¡s contenedores).  
- **ReplicaSet**: mantiene el nÃºmero de pods.  
- **Namespace**: segmentaciÃ³n lÃ³gica de recursos.  
- **CNI/CSI**: interfaces de red/almacenamiento para plugins.  
- **GitOps**: manifiestos en Git como **fuente de verdad** y reconciliaciÃ³n continua.

> **Nota:** Este marco es **independiente del proveedor** (Kubernetes, Nomad, Swarm). La demo del repositorio se ejecutarÃ¡ en un **clÃºster local** con manifiestos YAML estÃ¡ndar.

## **DEMO**



[â¬…ï¸ Anterior](../0.6.3/0.6.3.md) | [ğŸ  Home](../../../README.md) | [Siguiente â¡ï¸](../0.6.5/0.6.5.md)