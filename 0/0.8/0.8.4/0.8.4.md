> [0. Acerca del Grupo](../../0.md) ‚Ä∫ [0.8. Temas Individuales (Parte 2)](../0.8.md) ‚Ä∫ [0.8.4. Integrante 4](0.8.4.md)

# 0.8.4. Carolina Meza

## üßëüèª‚Äçüíª Desarrollo Conceptual

### ¬øQu√© es la Explicabilidad en Modelos de IA?

La **Explicabilidad en Modelos de Inteligencia Artificial (IA)** se refiere a la capacidad de entender y explicar las decisiones tomadas por un modelo de IA. En otras palabras, es el proceso de hacer que los resultados de un modelo sean comprensibles para los humanos, permitiendo que los usuarios o desarrolladores puedan seguir c√≥mo se llegaron a las conclusiones del modelo. Este concepto es fundamental para **garantizar la transparencia**, **confianza** y **justicia** en los sistemas basados en IA, especialmente en aplicaciones sensibles como la medicina, las finanzas o la justicia.

### üéØ Objetivos de la Explicabilidad en Modelos de IA

- **Mejorar la transparencia:** Hacer que el funcionamiento interno del modelo sea entendible.
- **Generar confianza:** Asegurar que los usuarios comprendan c√≥mo y por qu√© se tomaron las decisiones.
- **Identificar y corregir sesgos:** Detectar posibles prejuicios en los modelos y tomar acciones correctivas.
- **Cumplir con regulaciones:** Satisfacer normativas como GDPR que requieren explicaciones claras sobre las decisiones automatizadas.
- **Mejorar la toma de decisiones:** Permitir que los humanos comprendan y validen los resultados para tomar decisiones informadas.

### üß± Arquitectura Conceptual  
*(pir√°mide conceptual de niveles)*

| Capa / Nivel                | Qu√© Representa                                      | Ejemplo Aplicado                                                   |
|-----------------------------|-----------------------------------------------------|-------------------------------------------------------------------|
| **Datos de Entrada**        | Informaci√≥n utilizada para entrenar el modelo      | Im√°genes de radiograf√≠as para diagn√≥stico m√©dico                  |
| **Modelo de IA**            | Algoritmo que procesa y genera la predicci√≥n        | Red neuronal convolucional (CNN) para clasificaci√≥n de im√°genes   |
| **M√©todo de Explicaci√≥n**   | T√©cnicas usadas para desentra√±ar el modelo         | SHAP (Shapley Additive Explanations), LIME (Local Interpretable Model-agnostic Explanations) |
| **Interpretaci√≥n de Resultados** | An√°lisis humano de las explicaciones generadas | Visualizaci√≥n de las caracter√≠sticas clave que impactaron la predicci√≥n |
| **Validaci√≥n y Justificaci√≥n** | Asegurar que la explicaci√≥n sea adecuada         | Evaluaci√≥n de la fidelidad de las explicaciones mediante auditor√≠as externas |

> **Importante:** La explicabilidad no solo se enfoca en qu√© hace el modelo, sino tambi√©n en por qu√© y c√≥mo llega a sus decisiones.

### üìä M√©tricas Clave

- **Interpretabilidad:** Facilidad con la que los resultados del modelo pueden ser comprendidos por los humanos.  
- **Consistencia:** Estabilidad de las explicaciones ante datos similares.  
- **Fidelidad:** Grado en que la explicaci√≥n refleja el comportamiento real del modelo.  
- **Precisi√≥n de la Explicaci√≥n:** Cu√°nto de la predicci√≥n puede explicarse de manera clara y coherente.  
- **Impacto del Modelo:** Nivel de confianza que los usuarios ganan al comprender el proceso de toma de decisiones del modelo.

### üõ†Ô∏è Criterios para Dise√±ar Explicabilidad en Modelos de IA

- **Seleccionar el tipo de modelo adecuado:** Algunos modelos son naturalmente m√°s explicables que otros (p. ej., √°rboles de decisi√≥n vs. redes neuronales profundas).
- **Definir las variables clave:** Identificar qu√© factores son relevantes para la toma de decisiones del modelo.
- **Elegir t√©cnicas de explicaci√≥n apropiadas:** M√©todos como LIME, SHAP o representaciones visuales para hacer el modelo m√°s comprensible.
- **Garantizar la transparencia:** Hacer visible el funcionamiento interno del modelo sin perder precisi√≥n.
- **Automatizar la generaci√≥n de explicaciones:** Integrar la explicabilidad dentro del pipeline de desarrollo y en la interfaz de usuario.
- **Verificar la justicia y la √©tica:** Asegurarse de que las explicaciones sean justas, sin sesgos discriminatorios.

## üëæ Consideraciones T√©cnicas

En t√©rminos pr√°cticos, **la explicabilidad en IA** es comparable a **un asistente que explica decisiones complejas**:

- **Modelo de caja negra**: Un modelo de IA sin explicabilidad es como un "caja negra", donde la entrada va, pero no sabemos exactamente c√≥mo se toma la decisi√≥n.
- **Modelo explicable**: Con explicabilidad, es como tener un "consejero" que te muestra el camino que tom√≥ para llegar a la conclusi√≥n final, detallando los puntos importantes que influyeron en la decisi√≥n.

Estas consideraciones est√°n pensadas para equipos que comienzan a integrar IA explicable en sus sistemas y buscan aumentar la confianza y cumplimiento en sistemas automatizados cr√≠ticos.

---
## Demo
## 1. Requisitos Previos

Antes de comenzar, necesitas instalar lo siguiente:

1. **Python**  
   - Aseg√∫rate de tener Python instalado en tu sistema.  
   - Para verificar que est√° instalado, abre una terminal y ejecuta:
     ```bash
     python --version
     pip --version
     ```

2. **Editor de c√≥digo**  
   - Se recomienda **Visual Studio Code (VSCode)** para el desarrollo.  
   - Plugins recomendados: Python, Jupyter, Pylance.

## 2. Crear un Entorno Virtual

1. Abre la terminal o PowerShell.
2. Crea un entorno virtual y act√≠valo:
    ```bash
    python -m venv venv
    source venv/bin/activate   # Para Linux o macOS
    venv\Scripts\activate      # Para Windows
    ```

## 3. Instalar Librer√≠as Necesarias

1. Instala las librer√≠as necesarias para el proyecto:
    ```bash
    pip install lime scikit-learn numpy pandas matplotlib
    ```

2. Instala Jupyter Notebook para ejecutar el demo interactivo:
    ```bash
    pip install jupyterlab
    ```
    Instala flask gaa
3. pip install flask lime matplotlib scikit-learn pandas

## 4. Estructura de Archivos Recomendados

    demo-explicabilidad-ia/
    ‚îú‚îÄ‚îÄ data/
    ‚îÇ   ‚îî‚îÄ‚îÄ dataset.csv
    ‚îú‚îÄ‚îÄ notebooks/
    ‚îÇ   ‚îî‚îÄ‚îÄ demo-lime.ipynb
    ‚îú‚îÄ‚îÄ model/
    ‚îÇ   ‚îî‚îÄ‚îÄ model.py
    ‚îú‚îÄ‚îÄ app.py
    ‚îî‚îÄ‚îÄ requirements.txt
    ![alt text](image.png)

## 5. Cargar el Dataset y Entrenar el Modelo

5.1 En el archivo model.py, se utiliza el conjunto de datos Iris de scikit-learn para entrenar un modelo de clasificaci√≥n utilizando Random Forest:

```python
# model.py
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import train_test_split
from sklearn.datasets import load_iris
import pandas as pd

# Cargar el dataset Iris
data = load_iris()
X = pd.DataFrame(data.data, columns=data.feature_names)
y = pd.Series(data.target)

# Dividir el dataset en entrenamiento y prueba
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

# Entrenar el modelo
model = RandomForestClassifier(n_estimators=100, random_state=42)
model.fit(X_train, y_train)

# Evaluar el modelo
accuracy = model.score(X_test, y_test)
print(f'Accuracy del modelo: {accuracy}')

5.2 LIME (Local Interpretable Model-Agnostic Explanations) se utiliza para explicar c√≥mo el modelo hace sus predicciones. Se configura un LimeTabularExplainer para explicar las predicciones de la clasificaci√≥n:

```python
import lime.lime_tabular

# Crear el explicador LIME
explainer = lime.lime_tabular.LimeTabularExplainer(
    training_data=X_train.values,
    training_labels=y_train.values,
    mode='classification',
    class_names=['Setosa', 'Versicolor', 'Virginica'],
    feature_names=X_train.columns,
    discretize_continuous=True
)

5.3 Desarrollar la API con Flask: 
Se crea un servidor con Flask que expone dos rutas para realizar predicciones y generar explicaciones:
- Ruta /predict: Recibe una solicitud POST con las caracter√≠sticas de una instancia y devuelve la predicci√≥n del modelo.
- Ruta /explanation: Recibe una solicitud POST con las caracter√≠sticas de una instancia y devuelve una explicaci√≥n generada por LIME.

```python 
ffrom flask import Flask, jsonify, request
import numpy as np
import pandas as pd
from model.model import model, X_train, y_train
import lime
import lime.lime_tabular
import matplotlib.pyplot as plt
from sklearn.datasets import load_iris

app = Flask(__name__)

@app.route('/')
def home():
    return "Welcome to the AI Prediction and Explanation API!"

# Crear el explicador LIME
explainer = lime.lime_tabular.LimeTabularExplainer(
    training_data=X_train.values,
    training_labels=y_train.values,
    mode='classification',
    class_names=['Setosa', 'Versicolor', 'Virginica'],
    feature_names=X_train.columns,
    discretize_continuous=True
)

@app.route('/predict', methods=['POST'])
def predict():
    # Obtener los datos del cuerpo de la solicitud
    data = request.get_json()
    instance = np.array(data['features']).reshape(1, -1)
    
    # Realizar la predicci√≥n
    prediction = model.predict(instance)
    predicted_class = prediction[0]
    class_names = ['Setosa', 'Versicolor', 'Virginica']
    predicted_label = class_names[predicted_class]
    
    return jsonify({'prediction': predicted_label})

@app.route('/explanation', methods=['POST'])
def explanation():
    # Obtener los datos del cuerpo de la solicitud
    data = request.get_json()
    instance = np.array(data['features']).reshape(1, -1)
    
    # Generar la explicaci√≥n con LIME
    explanation = explainer.explain_instance(instance[0], model.predict_proba, num_features=4)
    
    # Mostrar la explicaci√≥n con matplotlib
    fig = explanation.as_pyplot_figure()
    plt.show()

    return jsonify({'message': 'Explanation displayed successfully!'})

if __name__ == '__main__':
    app.run(debug=True, host='0.0.0.0', port=5000)

5.4 niciar el Servidor Flask :
Ejecuta el archivo app.py para iniciar el servidor Flask: python app.py
--- El servidor Flask estar√° disponible en http://localhost:5000.
![alt text](image-2.png)

5.5 Realizar Solicitudes a la API: 
Env√≠a una solicitud POST a la ruta /predict con los datos de una instancia para hacer la predicci√≥n:
Ejemplo de solicitud (usando Postman):

{
  "features": [5.1, 3.5, 1.4, 0.2]
}

respuesta esperada: 
{
  "prediction": "Setosa"
}
![alt text](image-3.png)


5.6 Obtener una Explicaci√≥n: 
Env√≠a una solicitud POST a la ruta /explanation para obtener la explicaci√≥n generada por LIME para una instancia espec√≠fica:
![alt text](image-1.png)
-Resultado: El gr√°fico de explicaci√≥n de LIME se mostrar√° utilizando matplotlib.

## DEMO GAA

Link de Github del Proyecto:
https://github.com/arqui252-meza-beraun/TRABAJO INDIVIDUAL ARQUI

[‚¨ÖÔ∏è Anterior](../0.8.3/0.8.3.md) | [üè† Home](../../../README.md) | [Siguiente ‚û°Ô∏è](../0.8.5/0.8.5.md)